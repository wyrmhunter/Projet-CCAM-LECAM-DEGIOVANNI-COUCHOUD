{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset study and objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to the dataset folder\n",
    "dos_path = './_data/dos_dataset'\n",
    "\n",
    "# Finding the JSON files\n",
    "file_paths = glob.glob(os.path.join(dos_path, 'traceJSON-*-*.json'))\n",
    "\n",
    "# Initialising a list to stock the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Browsing the JSON files\n",
    "for file_path in file_paths:\n",
    "    # Extracting the receiver id in the name of the file\n",
    "    file_name= os.path.basename(file_path)\n",
    "    receiver_id = file_name.split('-')[1]\n",
    "    \n",
    "    # Reading the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    # Stripping each JSON line and adding the receiver id\n",
    "    json_data = [json.loads(line.strip()) for line in data]\n",
    "    for line in json_data:\n",
    "        line['receiver_id'] = receiver_id\n",
    "    \n",
    "    # Creating a pandas dataframe\n",
    "    df = pd.DataFrame(json_data)\n",
    "    \n",
    "    # Filtering the lines where type = 3\n",
    "    df_filtered = df[df['type'] == 3]\n",
    "    \n",
    "    # Adding the filtered dataframe to the dataframe list\n",
    "    dfs.append(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['type', 'rcvTime', 'receiver_id', 'sendTime', 'sender', 'senderPseudo',\n",
      "       'messageID', 'label', 'pos1', 'pos2', 'pos3', 'pos_noise1',\n",
      "       'pos_noise2', 'pos_noise3', 'spd1', 'spd2', 'spd3', 'spd_noise1',\n",
      "       'spd_noise2', 'spd_noise3', 'acl1', 'acl2', 'acl3', 'acl_noise1',\n",
      "       'acl_noise2', 'acl_noise3', 'hed1', 'hed2', 'hed3', 'hed_noise1',\n",
      "       'hed_noise2', 'hed_noise3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sender_labels_path = './_data/sender_labels.csv'\n",
    "\n",
    "df_labels = pd.read_csv(sender_labels_path, delimiter=';')\n",
    "\n",
    "# Columns to split\n",
    "columns_to_split = ['pos', 'pos_noise', 'spd', 'spd_noise', 'acl', 'acl_noise', 'hed', 'hed_noise']\n",
    "\n",
    "for i in range(5):\n",
    "    dfs[i] = pd.merge(dfs[i], df_labels, how='left', on='sender')\n",
    "\n",
    "    # Function to split lists into sub-columns\n",
    "    def split_list_to_columns(row, col_name):\n",
    "        return pd.Series(row[col_name])\n",
    "\n",
    "    # Applying the function to every columns that have list data\n",
    "    for col in columns_to_split:\n",
    "        new_cols = dfs[i].apply(lambda x: split_list_to_columns(x, col), axis=1)\n",
    "        new_cols.columns = [f\"{col}{i+1}\" for i in range(len(dfs[i][col][0]))]\n",
    "        dfs[i][new_cols.columns] = new_cols\n",
    "    \n",
    "    dfs[i] = dfs[i].drop(columns=columns_to_split)\n",
    "\n",
    "\n",
    "# Printing the final dataframe\n",
    "\n",
    "print(dfs[0].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "X_train shape: (492, 31)\n",
      "X_test shape: (123, 31)\n",
      "y_train shape: (492,)\n",
      "y_test shape: (123,)\n",
      "\n",
      "Split 2:\n",
      "X_train shape: (948, 31)\n",
      "X_test shape: (238, 31)\n",
      "y_train shape: (948,)\n",
      "y_test shape: (238,)\n",
      "\n",
      "Split 3:\n",
      "X_train shape: (952, 31)\n",
      "X_test shape: (238, 31)\n",
      "y_train shape: (952,)\n",
      "y_test shape: (238,)\n",
      "\n",
      "Split 4:\n",
      "X_train shape: (433, 31)\n",
      "X_test shape: (109, 31)\n",
      "y_train shape: (433,)\n",
      "y_test shape: (109,)\n",
      "\n",
      "Split 5:\n",
      "X_train shape: (305, 31)\n",
      "X_test shape: (77, 31)\n",
      "y_train shape: (305,)\n",
      "y_test shape: (77,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to create training and test splits for each dataframe\n",
    "def create_splits(dfs, test_size=0.2, random_state=42):\n",
    "    train_test_splits = []\n",
    "    for df in dfs:\n",
    "        # Split dataframe into features (X) and target variable (y)\n",
    "        X = df.drop(columns=['label'])  # Replace 'target_column' with the actual name of your target column\n",
    "        y = df['label']  # Replace 'target_column' with the actual name of your target column\n",
    "        \n",
    "        # Perform train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Store the splits in a tuple and append to the list\n",
    "        train_test_splits.append((X_train, X_test, y_train, y_test))\n",
    "    \n",
    "    return train_test_splits\n",
    "\n",
    "# Create training and test splits for each dataframe\n",
    "train_test_splits = create_splits(dfs)\n",
    "\n",
    "# Accessing splits for each dataframe\n",
    "for i, (X_train, X_test, y_train, y_test) in enumerate(train_test_splits):\n",
    "    print(f\"Split {i+1}:\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of data in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model chosen for our task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
