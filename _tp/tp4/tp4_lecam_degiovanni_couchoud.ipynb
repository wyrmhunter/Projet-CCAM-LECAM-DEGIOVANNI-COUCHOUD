{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP4"
      ],
      "metadata": {
        "id": "swh76TMl02s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Vehicular ad hoc networks (VANET), it is crucial to be able to detect misbehaviors, and in particular cyberattacks, to guarantee the safety of the drivers.\n",
        "\n",
        "Denial-of-service (DoS) attacks send malicious traffic of data through a great number of attack machines in an attempt to shut down the availability of information for vehicles to communicate. This work will focus on predicting those specific attacks using Federated Learning.\n",
        "\n",
        "Federated Learning will be implemented using the Tensorflow and the Tensorflow Federated libraries. The Pandas library will be used to clean and preprocess the dataset."
      ],
      "metadata": {
        "id": "85lFgJDCRLZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "wfd-SC6N064e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used is a public simulated dataset called Vehicular Reference Misbehavior (VeReMi). This dataset presents several attack types such as Data Replay or Eventual Stop, but we will focus on DoS attacks in this work.\n",
        "\n",
        "In the *_data* folder, you will find another folder *dos_dataset* and a CSV file containing the sender-labels correspondences. The *dos_dataset* contains the JSON files with all the message data."
      ],
      "metadata": {
        "id": "WBYqpizwTlFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset study and objectives"
      ],
      "metadata": {
        "id": "1B0nzbkB1AJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the JSON files, the messages can be of 3 types :\n",
        "- Type = 2 : GPS, sent messages\n",
        "- Type = 3 : BSM (Basic Safety Message), received messages\n",
        "- Type = 4 : Ground truth, received messages\n",
        "We will therefore focus on the type 3 received messages.\n",
        "\n",
        "The type 3 messages have the following data :\n",
        "- *rcvTime* : float\n",
        "- *sendTime* : float\n",
        "- *sender* : int, corresponds to the *sender* column in *sender_labels.csv*. We will be able to join the files on this column to associate messages with their labels.\n",
        "- *senderPseudo* : int\n",
        "- *messageID* : int\n",
        "- *pos & noise* : lists of floats\n",
        "- *spd & noise* : lists of floats\n",
        "- *acl & noise* : lists of floats\n",
        "- *hed & noise* : lists of floats\n",
        "\n",
        "In each file name, we can note the receiver id (first number in the file name). In the dataframe that we will create further, a column with this id will be added to make the Federated Learning approach possible, as each receiver will train its own model.\n",
        "\n",
        "**The main goal will be to predict the attack label (0 for a normal vehicle, 1 for an attacking vehicle).**"
      ],
      "metadata": {
        "id": "BMo0r2xuU8i3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the variables"
      ],
      "metadata": {
        "id": "vjl7Xk001B5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset cleaning"
      ],
      "metadata": {
        "id": "G-qTFsOv1EdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first goal will be to clean the dataset by stripping the JSON files and creating one big pandas dataframe that will contains all of the data.\n",
        "\n",
        "The lines will be filtered so that only type 3 messages remain and a *receiver_id* column corresponding to the first number in the file names will be added."
      ],
      "metadata": {
        "id": "1cKzFIhZYezM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to the dataset folder\n",
        "dos_path = './_data/dos_dataset'\n",
        "\n",
        "# Finding the JSON files\n",
        "file_paths = glob.glob(os.path.join(dos_path, 'traceJSON-*-*.json'))\n",
        "\n",
        "# Initialising a list to stock the dataframes\n",
        "dfs = []\n",
        "\n",
        "# Browsing the JSON files\n",
        "for file_path in file_paths:\n",
        "    # Extracting the receiver id in the name of the file\n",
        "    file_name= os.path.basename(file_path)\n",
        "    receiver_id = file_name.split('-')[1]\n",
        "\n",
        "    # Reading the JSON file\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "    # Stripping each JSON line and adding the receiver id\n",
        "    json_data = [json.loads(line.strip()) for line in data]\n",
        "    for line in json_data:\n",
        "        line['receiver_id'] = receiver_id\n",
        "\n",
        "    # Creating a pandas dataframe\n",
        "    df = pd.DataFrame(json_data)\n",
        "\n",
        "    # Filtering the lines where type = 3\n",
        "    df_filtered = df[df['type'] == 3]\n",
        "\n",
        "    # Adding the filtered dataframe to the dataframe list\n",
        "    dfs.append(df_filtered)\n",
        "\n",
        "df_final = pd.concat(dfs)"
      ],
      "metadata": {
        "id": "2vw5TPE21HCU"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further processing"
      ],
      "metadata": {
        "id": "AeHQjOT01MsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step will be to make the join between the sender-labels correspondences file and our dataframe on the *sender* column. Also, we will have to split the columns that contain lists of floats so that every column contains floats and only floats. For example, the *pos* column will be separated into 3 columns *pos1*, *pos2* and *pos3*."
      ],
      "metadata": {
        "id": "zjITPSy-ZUi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sender_labels_path = './_data/sender_labels.csv'\n",
        "\n",
        "df_labels = pd.read_csv(sender_labels_path, delimiter=';')\n",
        "\n",
        "# Columns to split\n",
        "columns_to_split = ['pos', 'pos_noise', 'spd', 'spd_noise', 'acl', 'acl_noise', 'hed', 'hed_noise']\n",
        "\n",
        "\n",
        "df_final = pd.merge(df_final, df_labels, how='left', on='sender')\n",
        "\n",
        "# Function to split lists into sub-columns\n",
        "def split_list_to_columns(row, col_name):\n",
        "    return pd.Series(row[col_name])\n",
        "\n",
        "# Applying the function to every columns that have list data\n",
        "for col in columns_to_split:\n",
        "    new_cols = df_final.apply(lambda x: split_list_to_columns(x, col), axis=1)\n",
        "    new_cols.columns = [f\"{col}{i+1}\" for i in range(len(df_final[col][0]))]\n",
        "    df_final[new_cols.columns] = new_cols\n",
        "\n",
        "df_final = df_final.drop(columns=columns_to_split)\n",
        "\n",
        "\n",
        "# Printing the final dataframe\n",
        "\n",
        "print(df_final.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlH3h2zw1PzU",
        "outputId": "d1eb57d7-3d63-4f3b-cece-437c97836424"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['type', 'rcvTime', 'receiver_id', 'sendTime', 'sender', 'senderPseudo',\n",
            "       'messageID', 'label', 'pos1', 'pos2', 'pos3', 'pos_noise1',\n",
            "       'pos_noise2', 'pos_noise3', 'spd1', 'spd2', 'spd3', 'spd_noise1',\n",
            "       'spd_noise2', 'spd_noise3', 'acl1', 'acl2', 'acl3', 'acl_noise1',\n",
            "       'acl_noise2', 'acl_noise3', 'hed1', 'hed2', 'hed3', 'hed_noise1',\n",
            "       'hed_noise2', 'hed_noise3'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before getting into the Federated Model, we will have to transform our dataframe into a ClientData object (*tff.simulation.datasets.ClientData* class). The *receiver_id* column will be use to create client ids and to group clients. At the end of this process, each client will have a dataset containing tensors representing their data and labels."
      ],
      "metadata": {
        "id": "gM_-eRevag9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Drop rows with NaN values\n",
        "df = df_final.dropna()\n",
        "\n",
        "# Function to create client datasets\n",
        "def create_client_data():\n",
        "    client_data = []\n",
        "    # Assuming 'receiver_id' is the column to group clients\n",
        "    for _, client_df in df.groupby('receiver_id'):\n",
        "        client_data.append(\n",
        "            tf.data.Dataset.from_tensor_slices(\n",
        "                ({'dense_28_input': client_df.drop(columns=['label']).values.astype('float32')},\n",
        "                 client_df['label'].values.astype('float32'))\n",
        "            )\n",
        "        )\n",
        "    return client_data\n",
        "\n",
        "# Create ClientData object directly\n",
        "client_data = create_client_data()\n",
        "\n",
        "# Create ClientData object\n",
        "client_data_obj = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "    client_ids=[str(i) for i in range(len(client_data))],  # Assuming client_ids are numeric\n",
        "    serializable_dataset_fn=lambda client_id: client_data[int(client_id)]\n",
        ")\n",
        "\n",
        "# Test by printing the client data for the first client\n",
        "client_dataset = client_data_obj.create_tf_dataset_for_client(client_data_obj.client_ids[0])\n",
        "for data in client_dataset.take(1):\n",
        "    print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z9CNgPI5T2L",
        "outputId": "db5e0d28-a8f7-4128-af09-c392e2dbf90b"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'dense_28_input': <tf.Tensor: shape=(31,), dtype=float32, numpy=\n",
            "array([ 3.00000000e+00,  5.04046016e+04,  1.50000000e+01,  5.04046016e+04,\n",
            "        9.00000000e+00,  1.09500000e+03,  8.48800000e+03,  1.31761047e+03,\n",
            "        1.01347186e+03,  0.00000000e+00,  3.60492206e+00,  3.79965401e+00,\n",
            "        0.00000000e+00, -9.05958951e-01, -1.42745638e+00,  0.00000000e+00,\n",
            "       -1.89850503e-03, -2.99134199e-03,  0.00000000e+00, -9.18970764e-01,\n",
            "       -1.44807351e+00,  0.00000000e+00,  3.99408024e-03,  5.08691743e-03,\n",
            "        0.00000000e+00, -5.35855293e-01, -8.44309807e-01,  0.00000000e+00,\n",
            "        2.02444115e+01,  2.13281651e+01,  0.00000000e+00], dtype=float32)>}, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Learning Model"
      ],
      "metadata": {
        "id": "FqVv5VfRQnaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the machine learning method used, we will try two methods : Convolutionnal Neural Network and Logistic Regression. Weight updates can be challenging with CNNs due to their large number of parameters, whereas Logistic Regression allows an easier weight update with its smaller number of parameters.\n",
        "Earlier during the dataset preprocessing, we partitioned the data using the ids of the receivers. To perform the model aggregation, we will use federated averaging with the Tensorflow Federate library."
      ],
      "metadata": {
        "id": "o4jLzrBocwFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "XZI5e-rsfpHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "# Load simulation data.\n",
        "source = client_data_obj\n",
        "def make_client_data(n):\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [make_client_data(n) for n in range(5)]\n",
        "\n",
        "# Wrap a Keras CNN model for use with TFF.\n",
        "keras_model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(\n",
        "    2, tf.nn.softmax, input_shape=(31,), kernel_initializer='zeros')\n",
        "])\n",
        "\n",
        "tff_model = tff.learning.models.functional_model_from_keras(\n",
        "      keras_model,\n",
        "      loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      metrics_constructor=collections.OrderedDict(\n",
        "        accuracy=tf.keras.metrics.SparseCategoricalAccuracy))\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "trainer = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "  tff_model,\n",
        "  client_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=0.1))\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  result = trainer.next(state, train_data)\n",
        "  state = result.state\n",
        "  metrics = result.metrics\n",
        "  print(metrics['client_work']['train']['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq7p-7jOdiM-",
        "outputId": "0bd1a60d-2504-40e2-aab7-eef4ccb75dad"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.46079183\n",
            "0.3706258\n",
            "0.3706258\n",
            "0.3706258\n",
            "0.3706258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "HDzjjJxyfucg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "# Load simulation data.\n",
        "source = client_data_obj\n",
        "def make_client_data(n):\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [make_client_data(n) for n in range(5)]\n",
        "\n",
        "# Wrap a Keras CNN model for use with TFF.\n",
        "keras_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(16, 3, activation='relu', input_shape=(31,1)),\n",
        "    tf.keras.layers.MaxPooling1D(2),\n",
        "    tf.keras.layers.Conv1D(8, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=2, activation='softmax')\n",
        "])\n",
        "tff_model = tff.learning.models.functional_model_from_keras(\n",
        "      keras_model,\n",
        "      loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      metrics_constructor=collections.OrderedDict(\n",
        "        accuracy=tf.keras.metrics.SparseCategoricalAccuracy))\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "trainer = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "  tff_model,\n",
        "  client_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=0.1))\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  result = trainer.next(state, train_data)\n",
        "  state = result.state\n",
        "  metrics = result.metrics\n",
        "  print(metrics['client_work']['train']['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5JzlZql1q2f",
        "outputId": "97be2052-da30-4f6d-d630-33c086476084"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5634738\n",
            "0.5634738\n",
            "0.5634738\n",
            "0.5634738\n",
            "0.5634738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions"
      ],
      "metadata": {
        "id": "E_E4zuHffwo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNNs presents a better final accuracy (0.56) compared to Logistic Regression (0.37). Even if the weight updates will be more difficult at a larger scale of data, it would be a significant gain in accuracy to use CNNs over Logistic Regression.\n",
        "\n",
        "However, the accuracy using CNNs is still low. It could be because the patterns in the data are too complex for the models, but at the same time the models should not be too complex to not make weight updates too difficult. Further work should be made to find that trade-off between accuracy and model complexity."
      ],
      "metadata": {
        "id": "asczBT3dfzH4"
      }
    }
  ]
}